{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "d0 = np.loadtxt(\"./0.txt\", delimiter=\" \",dtype=np.float32)\n",
    "d1 = np.loadtxt(\"./1.txt\", delimiter=\" \",dtype=np.float32)\n",
    "\n",
    "pre_point0 = d0[:,0:6]\n",
    "next_point0 = d0[:,6:12]\n",
    "pre_point1 = d1[:,0:6]\n",
    "next_point1 = d1[:,6:12]\n",
    "\n",
    "anchor = 100\n",
    "rate=0.3\n",
    "\n",
    "\n",
    "train_p_0 = pre_point0[0:int(len(pre_point0)*rate)]\n",
    "train_n_0 = next_point0[0:int(len(pre_point0)*rate)]\n",
    "train_p_1 = pre_point1[0:int(len(pre_point1)*rate)]\n",
    "train_n_1 = next_point1[0:int(len(next_point1)*rate)]\n",
    "\n",
    "\n",
    "test_p_0 = pre_point0[-1*anchor:]\n",
    "test_n_0 = next_point0[-1*anchor:]\n",
    "test_p_1 = pre_point1[-1*anchor:]\n",
    "test_n_1 = next_point1[-1*anchor:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pre = tf.placeholder(dtype=tf.float32, shape = [None, 6], name = \"pre\")\n",
    "x_next = tf.placeholder(dtype=tf.float32, shape = [None, 6], name = \"next\")\n",
    "y = tf.placeholder(dtype=tf.float32, shape = [None, 1], name = \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1_pre = tf.layers.dense(inputs=x_pre,\n",
    "                          units=16,\n",
    "                          activation=tf.nn.relu,\n",
    "                          name=\"fc1_pre\")\n",
    "\n",
    "fc1_next = tf.layers.dense(inputs=x_next,\n",
    "                           units=64,\n",
    "                           activation=tf.nn.relu,\n",
    "                           name=\"fc1_next\")\n",
    "\n",
    "\n",
    "fc2 = tf.layers.dense(inputs=tf.concat([fc1_pre, fc1_next], 1),\n",
    "                      units = 128,\n",
    "                      activation=tf.nn.relu,\n",
    "                      name=\"fc2\")\n",
    "fc2_bn = tf.layers.batch_normalization(fc2)\n",
    "\n",
    "fc3 = tf.layers.dense(inputs=fc2_bn,\n",
    "                      units = 128,\n",
    "                      activation=tf.nn.relu,\n",
    "                      name=\"fc3\")\n",
    "fc3_bn = tf.layers.batch_normalization(fc3)\n",
    "\n",
    "fc4 = tf.layers.dense(inputs=fc3_bn,\n",
    "                      units = 1,\n",
    "                      activation=tf.nn.sigmoid,\n",
    "                      name=\"fc4\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = tf.contrib.layers.apply_regularization(tf.contrib.layers.l2_regularizer(1e-4), tf.trainable_variables())\n",
    "\n",
    "loss = tf.reduce_sum(tf.square(fc4 -  y))\n",
    "\n",
    "out = tf.round(fc4)\n",
    "acc = tf.reduce_mean(tf.cast(tf.equal(out, y), tf.float32))\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(0.01).minimize(loss+reg)\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(300):\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    for i in range(10):\n",
    "        index_0 = np.random.choice(train_n_0.shape[0], 5000,replace=False)\n",
    "        index_1 = np.random.choice(train_n_1.shape[0], 5000,replace=True)\n",
    "        batch_train_p = np.concatenate((train_p_0[index_0],train_p_1[index_1]),0)\n",
    "        batch_train_n = np.concatenate((train_n_0[index_0],train_n_1[index_1]),0)\n",
    "        batch_label = np.concatenate((np.zeros(shape=[5000,1]), np.ones(shape=[5000,1])),0)\n",
    "        id_shuffer = np.random.choice(10000, 10000,replace=False)\n",
    "\n",
    "        l, _, a, o = sess.run([loss, train_step, acc, out], feed_dict={x_pre:batch_train_p[id_shuffer], x_next:batch_train_n[id_shuffer], y:batch_label[id_shuffer]})\n",
    "        train_loss.append(l)\n",
    "        train_acc.append(a)\n",
    "    print(\"train_loss = %3f, train_acc = %3f\"%(np.mean(train_loss), np.mean(train_acc)))\n",
    "    test_p = np.concatenate((test_p_0,test_p_1),0)\n",
    "    test_n = np.concatenate((test_n_0,test_n_1),0)\n",
    "    label = np.concatenate((np.zeros(shape=[anchor,1]), np.ones(shape=[anchor,1])),0)\n",
    "    idd_shuffer = np.random.choice(anchor*2, anchor*2, replace=False)\n",
    "    a,l,ou = sess.run([acc,loss,out] ,feed_dict={x_pre: test_p[idd_shuffer], x_next: test_n[idd_shuffer], y: label[idd_shuffer]})\n",
    "    print(\"test_acc = %3f\"%a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_3 = label[idd_shuffer]\n",
    "out_3 = ou\n",
    "from sklearn.metrics import confusion_matrix\n",
    "maxtrix = confusion_matrix(label_3, out_3,labels=[1,0])\n",
    "TP = maxtrix[0][0]\n",
    "TN = maxtrix[1][1]\n",
    "FP = maxtrix[1][0]\n",
    "FN = maxtrix[0][1]\n",
    "precision = 1.0*  TP  / (TP + FP)\n",
    "recall = 1.0 * TP/ (TP + FN)\n",
    "ACC = 1.0*(TP + TN) / (TP + TN + FP + FN)\n",
    "print('test : TP:%.3f;   TN:%.3f;      FP:%.3f;      FN:%.3f;  precision:%.3f  recall:%.3f  P:%.3f'%(TP, TN, FP, FN, precision, recall, ACC ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = 0.7\n",
    "\n",
    "train_p_0 = pre_point0[0:int(len(pre_point0)*rate)]\n",
    "train_n_0 = next_point0[0:int(len(pre_point0)*rate)]\n",
    "train_p_1 = pre_point1[0:int(len(pre_point1)*rate)]\n",
    "train_n_1 = next_point1[0:int(len(next_point1)*rate)]\n",
    "\n",
    "\n",
    "test_p_0 = pre_point0[-1*anchor:]\n",
    "test_n_0 = next_point0[-1*anchor:]\n",
    "test_p_1 = pre_point1[-1*anchor:]\n",
    "test_n_1 = next_point1[-1*anchor:]\n",
    "\n",
    "\n",
    "for k in range(300):\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    for i in range(10):\n",
    "        index_0 = np.random.choice(train_n_0.shape[0], 5000,replace=False)\n",
    "        index_1 = np.random.choice(train_n_1.shape[0], 5000,replace=True)\n",
    "        batch_train_p = np.concatenate((train_p_0[index_0],train_p_1[index_1]),0)\n",
    "        batch_train_n = np.concatenate((train_n_0[index_0],train_n_1[index_1]),0)\n",
    "        batch_label = np.concatenate((np.zeros(shape=[5000,1]), np.ones(shape=[5000,1])),0)\n",
    "        id_shuffer = np.random.choice(10000, 10000,replace=False)\n",
    "\n",
    "        l, _, a, o = sess.run([loss, train_step, acc, out], feed_dict={x_pre:batch_train_p[id_shuffer], x_next:batch_train_n[id_shuffer], y:batch_label[id_shuffer]})\n",
    "        train_loss.append(l)\n",
    "        train_acc.append(a)\n",
    "    print(\"train_loss = %3f, train_acc = %3f\"%(np.mean(train_loss), np.mean(train_acc)))\n",
    "    test_p = np.concatenate((test_p_0,test_p_1),0)\n",
    "    test_n = np.concatenate((test_n_0,test_n_1),0)\n",
    "    label = np.concatenate((np.zeros(shape=[anchor,1]), np.ones(shape=[anchor,1])),0)\n",
    "    idd_shuffer = np.random.choice(anchor*2, anchor*2, replace=False)\n",
    "    a,l,ou = sess.run([acc,loss,out] ,feed_dict={x_pre: test_p[idd_shuffer], x_next: test_n[idd_shuffer], y: label[idd_shuffer]})\n",
    "    print(\"test_acc = %3f\"%a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_7 = label[idd_shuffer]\n",
    "out_7 = ou\n",
    "from sklearn.metrics import confusion_matrix\n",
    "maxtrix = confusion_matrix(label_7, out_7,labels=[1,0])\n",
    "TP = maxtrix[0][0]\n",
    "TN = maxtrix[1][1]\n",
    "FP = maxtrix[1][0]\n",
    "FN = maxtrix[0][1]\n",
    "precision = 1.0*  TP  / (TP + FP)\n",
    "recall = 1.0 * TP/ (TP + FN)\n",
    "ACC = 1.0*(TP + TN) / (TP + TN + FP + FN)\n",
    "print('test : TP:%.3f;   TN:%.3f;      FP:%.3f;      FN:%.3f;  precision:%.3f  recall:%.3f  P:%.3f'%(TP, TN, FP, FN, precision, recall, ACC ))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = 0.9\n",
    "\n",
    "train_p_0 = pre_point0[0:int(len(pre_point0)*rate)]\n",
    "train_n_0 = next_point0[0:int(len(pre_point0)*rate)]\n",
    "train_p_1 = pre_point1[0:int(len(pre_point1)*rate)]\n",
    "train_n_1 = next_point1[0:int(len(next_point1)*rate)]\n",
    "\n",
    "\n",
    "test_p_0 = pre_point0[-1*anchor:]\n",
    "test_n_0 = next_point0[-1*anchor:]\n",
    "test_p_1 = pre_point1[-1*anchor:]\n",
    "test_n_1 = next_point1[-1*anchor:]\n",
    "\n",
    "\n",
    "for k in range(300):\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    for i in range(10):\n",
    "        index_0 = np.random.choice(train_n_0.shape[0], 5000,replace=False)\n",
    "        index_1 = np.random.choice(train_n_1.shape[0], 5000,replace=True)\n",
    "        batch_train_p = np.concatenate((train_p_0[index_0],train_p_1[index_1]),0)\n",
    "        batch_train_n = np.concatenate((train_n_0[index_0],train_n_1[index_1]),0)\n",
    "        batch_label = np.concatenate((np.zeros(shape=[5000,1]), np.ones(shape=[5000,1])),0)\n",
    "        id_shuffer = np.random.choice(10000, 10000,replace=False)\n",
    "\n",
    "        l, _, a, o = sess.run([loss, train_step, acc, out], feed_dict={x_pre:batch_train_p[id_shuffer], x_next:batch_train_n[id_shuffer], y:batch_label[id_shuffer]})\n",
    "        train_loss.append(l)\n",
    "        train_acc.append(a)\n",
    "    print(\"train_loss = %3f, train_acc = %3f\"%(np.mean(train_loss), np.mean(train_acc)))\n",
    "    test_p = np.concatenate((test_p_0,test_p_1),0)\n",
    "    test_n = np.concatenate((test_n_0,test_n_1),0)\n",
    "    label = np.concatenate((np.zeros(shape=[anchor,1]), np.ones(shape=[anchor,1])),0)\n",
    "    idd_shuffer = np.random.choice(anchor*2, anchor*2, replace=False)\n",
    "    a,l,ou = sess.run([acc,loss,out] ,feed_dict={x_pre: test_p[idd_shuffer], x_next: test_n[idd_shuffer], y: label[idd_shuffer]})\n",
    "    print(\"test_acc = %3f\"%a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_9 = label[idd_shuffer]\n",
    "out_9 = ou\n",
    "from sklearn.metrics import confusion_matrix\n",
    "maxtrix = confusion_matrix(label_9, out_9,labels=[1,0])\n",
    "TP = maxtrix[0][0]\n",
    "TN = maxtrix[1][1]\n",
    "FP = maxtrix[1][0]\n",
    "FN = maxtrix[0][1]\n",
    "precision = 1.0*  TP  / (TP + FP)\n",
    "recall = 1.0 * TP/ (TP + FN)\n",
    "ACC = 1.0*(TP + TN) / (TP + TN + FP + FN)\n",
    "print('test : TP:%.3f;   TN:%.3f;      FP:%.3f;      FN:%.3f;  precision:%.3f  recall:%.3f  P:%.3f'%(TP, TN, FP, FN, precision, recall, ACC ))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score,roc_curve\n",
    "\n",
    "label3 = np.uint8(label_3)\n",
    "pos_index = np.where(label3 == 1)\n",
    "pos_distance3 = np.array(out_3)[pos_index[0]]\n",
    "neg_index = np.where(label3 == 0)\n",
    "neg_distance3 = np.array(out_3)[neg_index[0]]\n",
    "plt.figure()\n",
    "plt.plot(list(range(100)), pos_distance3, 'r*', label='match-distance3')\n",
    "plt.plot(list(range(100)), neg_distance3, 'gp', label='nonmatch-distance3')\n",
    "# plt.legend()\n",
    "auc_ = roc_auc_score(label3, out_3)\n",
    "fpr3, tpr3, thresholds3 = roc_curve(label3, out_3)\n",
    "\n",
    "label7= np.uint8(label_7)\n",
    "pos_index = np.where(label7 == 1)\n",
    "pos_distance7 = np.array(out_7)[pos_index[0]]\n",
    "neg_index = np.where(label7 == 0)\n",
    "neg_distance7 = np.array(out_7)[neg_index[0]]\n",
    "plt.figure()\n",
    "plt.plot(list(range(100)), pos_distance7, 'r*', label='match-distance7')\n",
    "plt.plot(list(range(100)), neg_distance7, 'gp', label='nonmatch-distance7')\n",
    "# plt.legend()\n",
    "auc_ = roc_auc_score(label7, out_9)\n",
    "fpr7, tpr7, thresholds7 = roc_curve(label7, out_7)\n",
    "\n",
    "\n",
    "label9= np.uint8(label_9)\n",
    "pos_index = np.where(label9 == 1)\n",
    "pos_distance9 = np.array(out_9)[pos_index[0]]\n",
    "neg_index = np.where(label9 == 0)\n",
    "neg_distance9 = np.array(out_9)[neg_index[0]]\n",
    "plt.figure()\n",
    "plt.plot(list(range(100)), pos_distance9, 'r*', label='match-distance9')\n",
    "plt.plot(list(range(100)), neg_distance9, 'gp', label='nonmatch-distance9')\n",
    "# plt.legend()\n",
    "auc_ = roc_auc_score(label9, out_9)\n",
    "fpr9, tpr9, thresholds7 = roc_curve(label9, out_9)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr3, tpr3, 'r-',label='30_percent')\n",
    "plt.legend()\n",
    "plt.plot(fpr7, tpr7, 'b-',label='60_percent')\n",
    "plt.legend()\n",
    "plt.plot(fpr9, tpr9, 'g-',label='90_percent')\n",
    "plt.legend()\n",
    "plt.title('ROC-curve')\n",
    "plt.xlabel('FPR', fontsize=12)\n",
    "plt.ylabel('TPR', fontsize=12)\n",
    "plt.show()\n",
    "plt.savefig(\"./roc.jpg\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
